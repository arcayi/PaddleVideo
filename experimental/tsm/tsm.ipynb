{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 资源\n",
    "\n",
    "### **⭐ ⭐ ⭐ 欢迎点个小小的[Star](https://github.com/PaddlePaddle/awesome-DeepLearning/stargazers)支持！⭐ ⭐ ⭐**\n",
    "开源不易，希望大家多多支持~ \n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/c0fc093bffd84dc8920b33e8bf445bb0e842bc9fc29047878df03eb84691f0bf' width='700'></center>\n",
    "\n",
    "* 更多CV和NLP中的transformer模型(BERT、ERNIE、ViT、DeiT、Swin Transformer等)、深度学习资料，请参考：[awesome-DeepLearning](https://github.com/paddlepaddle/awesome-DeepLearning)\n",
    "\n",
    "\n",
    "* 更多视频模型(PP-TSM、PP-TSN、TimeSformer、BMN等)，请参考：[PaddleVideo](https://github.com/PaddlePaddle/PaddleVideo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. 实验介绍\n",
    "\n",
    "## 1.1 实验目的\n",
    "1. 理解并掌握TSM的改进模型PP-TSM的模型优化点；\n",
    "2. 熟悉如何基于飞桨开源框架构建PP-TSM模型，并进行模型训练、评估及推理等流程。\n",
    "\n",
    "## 1.2 实验内容\n",
    "随着互联网上视频的规模日益庞大，人们急切需要研究视频相关算法帮助人们更加容易地找到感兴趣内容的视频。而视频分类算法能够实现自动分析视频所包含的语义信息、理解其内容，对视频进行自动标注、分类和描述，达到与人媲美的准确率。视频分类是继图像分类问题后下一个急需解决的关键任务。\n",
    "\n",
    "视频分类的主要目标是理解视频中包含的内容，确定视频对应的几个关键主题。视频分类（Video Classification）算法将基于视频的语义内容如人类行为和复杂事件等，将视频片段自动分类至单个或多个类别。视频分类不仅仅是要理解视频中的每一帧图像，更重要的是要识别出能够描述视频的少数几个最佳关键主题。本实验将在视频分类数据集上给大家介绍经典的视频分类模型 TSM 的优化版本 PP-TSM。\n",
    "\n",
    "## 1.3 实验环境\n",
    "本实验使用aistudio至尊版GPU，cuda版本为10.1，具体依赖如下：\n",
    "* paddlepaddle-gpu==2.2.1.post101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/var/cache/buildkit/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://pypi.mirrors.ustc.edu.cn/simple/, http://192.168.1.10:7104/test/pypi/,http://192.168.1.10:7104/test/devpi/\n",
      "Looking in links: https://paddlepaddle.org.cn/whl/mkl/stable.html\n",
      "Requirement already satisfied: paddlepaddle-gpu in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (1.25.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (10.0.0)\n",
      "Requirement already satisfied: decorator in /home/vscode/.local/lib/python3.10/site-packages (from paddlepaddle-gpu) (5.1.1)\n",
      "Requirement already satisfied: astor in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (0.8.1)\n",
      "Requirement already satisfied: paddle-bfloat==0.1.7 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (0.1.7)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle-gpu) (3.20.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (0.17.3)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (3.4)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle-gpu) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->paddlepaddle-gpu) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->paddlepaddle-gpu) (3.7.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->paddlepaddle-gpu) (1.1.2)\n"
     ]
    }
   ],
   "source": [
    "# !python -m pip install paddlepaddle-gpu==2.2.1.post101 -f https://paddlepaddle.org.cn/whl/mkl/stable.html\n",
    "!python -m pip install paddlepaddle-gpu -f https://paddlepaddle.org.cn/whl/mkl/stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.4 实验设计\n",
    "实现方案如 **图2** 所示，对于一条输入的视频数据，首先使用卷积网络提取特征，获取特征表示；然后使用分类器获取属于每类视频动作的概率值。在训练阶段，通过模型输出的概率值与样本的真实标签构建损失函数，从而进行模型训练；在推理阶段，选出概率最大的类别作为最终的输出。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/66991b073132473cb1f71af1147c09c695b9d017404f4e9f9b6056ef5ad42556\" width=\"800\" hegiht=\"\" ></center>\n",
    "<center><br>图2 实现方案</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. 实现流程\n",
    "视频分类任务实现流程主要分为以下7个部分：\n",
    "1. 数据准备：根据网络接收的数据格式，完成相应的预处理操作，保证模型正常读取；\n",
    "1. 模型构建：视频分类模型构建；\n",
    "1. 训练配置：实例化模型，指定模型采用的寻解算法（优化器）；\n",
    "1. 模型训练：执行多轮训练不断调整参数，以达到较好的效果；\n",
    "1. 模型保存：保存模型参数；\n",
    "1. 模型评估：对训练好的模型进行评估测试，观察准确率和损失变化；\n",
    "1. 模型推理：使用一条视频数据验证模型分类效果；\n",
    "\n",
    "## 2.1 数据准备\n",
    "### 2.1.1 数据集简介\n",
    "[UCF101数据集](https://www.crcv.ucf.edu/data/UCF101.php) 是一个动作识别数据集，包含现实的动作视频，从 YouTube 上收集，有 101 个动作类别。该数据集是 UCF50 数据集的扩展，该数据集有 50 个动作类别。从 101 个动作类的 13320 个视频中，UCF101 给出了最大的多样性，并且在摄像机运动、物体外观和姿态、物体尺度、视点、杂乱背景、光照条件等方面存在较大的差异，这是目前极具挑战性的数据。\n",
    "\n",
    "由于大多数可用的动作识别数据集都不现实，而且是由参与者进行的，UCF101 旨在通过学习和探索新的现实行动类别来鼓励进一步研究行动识别。\n",
    "101 个动作类的视频中，动作类别可以分为5类，如**图3**中5种颜色的标注：\n",
    "* Human-Object Interaction\n",
    "* Body-Motion Only\n",
    "* Human-Human Interaction\n",
    "* Playing Musical Instruments\n",
    "* Sports\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/81d52c2b384048ef859a4fa338024418ffab6dce77df4e04812368f0b9cc85d5\" width=\"700\" hegiht=\"\" ></center>\n",
    "<center><br>图3 UCF101数据集</br></center>\n",
    "<br></br>\n",
    "\n",
    "### 2.1.2 数据文件简介\n",
    "将UCF101数据集解压存放在 `ucf101` 目录下，文件组织形式如下所示：\n",
    "```\n",
    "├── ucf101_{train,val}_videos.txt\n",
    "├── ucf101\n",
    "│   ├── ApplyEyeMakeup\n",
    "│   │   ├── v_ApplyEyeMakeup_g01_c01.avi\n",
    "│   │   └── ...\n",
    "│   ├── YoYo\n",
    "│   │   ├── v_YoYo_g25_c05.avi\n",
    "│   │   └── ...\n",
    "│   └── ...\n",
    "```\n",
    "其中，`ucf101_{train,val}_videos.txt` 中存放的是视频信息，部分内容展示如下：\n",
    "```\n",
    "./ucf101/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01 0\n",
    "./ucf101/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02 0\n",
    "./ucf101/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03 0\n",
    "```\n",
    "第一个元素表示视频文件路径，第二个元素表示该视频文件对应的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 仅在第一次运行代码时使用\n",
    "# 如不能正常运行，请将路径更新为data目录下的对应路径\n",
    "# !unzip -oq /home/aistudio/data/data105621/UCF101.zip \n",
    "# !mv UCF-101/ ucf101/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1.3 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LD_LIBRARY_PATH=/usr/lib/python3.10/dist-packages/cv2/../../../../lib/x86_64-linux-gnu:/opt/tritonserver/backends/onnxruntime:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-12.1/NsightSystems-cli-2023.2.1/host-linux-x64\n",
      "/usr/lib/python3.10/dist-packages/cv2/../../../../lib/x86_64-linux-gnu:/opt/tritonserver/backends/onnxruntime:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-12.1/NsightSystems-cli-2023.2.1/host-linux-x64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "%env LD_LIBRARY_PATH={os.environ['LD_LIBRARY_PATH']}:/usr/local/cuda-12.1/NsightSystems-cli-2023.2.1/host-linux-x64\n",
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.1/NsightSystems-cli-2023.2.1/host-linux-x64\n",
    "# !echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Can not import paddle core while this file exists: /usr/local/lib/python3.10/dist-packages/paddle/fluid/libpaddle.so\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "libssl.so.1.1: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpaddle\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/paddle/__init__.py:31\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbatch\u001b[39;00m \u001b[39mimport\u001b[39;00m batch  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# Do the *DUPLICATED* monkey-patch for the tensor object.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We need remove the duplicated code here once we fix\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# the illogical implement in the monkey-patch methods later.\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m monkey_patch_variable\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m monkey_patch_math_tensor\n\u001b[1;32m     34\u001b[0m monkey_patch_variable()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/paddle/framework/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m# TODO: import framework api under this directory\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m random  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrandom\u001b[39;00m \u001b[39mimport\u001b[39;00m seed  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m get_default_dtype  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/paddle/framework/random.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m# TODO: define random api\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpaddle\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpaddle\u001b[39;00m \u001b[39mimport\u001b[39;00m fluid\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpaddle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfluid\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[1;32m     20\u001b[0m __all__ \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/paddle/fluid/__init__.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     35\u001b[0m \u001b[39m# import all class inside framework into fluid module\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m framework\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39m# import all class inside executor into fluid module\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/paddle/fluid/framework.py:35\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m \u001b[39mimport\u001b[39;00m framework_pb2, data_feed_pb2\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m unique_name\n\u001b[1;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpaddle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mfluid_version\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/paddle/fluid/core.py:356\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m avx_supported() \u001b[39mand\u001b[39;00m libpaddle\u001b[39m.\u001b[39mis_compiled_with_avx():\n\u001b[1;32m    352\u001b[0m         sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mwrite(\n\u001b[1;32m    353\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mError: Your machine doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt support AVX, but the installed PaddlePaddle is avx core, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myou should reinstall paddlepaddle with no-avx core.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m         )\n\u001b[0;32m--> 356\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_paddle_custom_device_lib_path\u001b[39m(lib_path):\n\u001b[1;32m    360\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mCUSTOM_DEVICE_ROOT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         \u001b[39m# use setted environment value\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/paddle/fluid/core.py:269\u001b[0m\n\u001b[1;32m    266\u001b[0m             sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39mError: Can not preload libgomp.so\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m libpaddle\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m avx_supported() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m libpaddle\u001b[39m.\u001b[39mis_compiled_with_avx():\n\u001b[1;32m    272\u001b[0m         sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mwrite(\n\u001b[1;32m    273\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mHint: Your machine support AVX, but the installed paddlepaddle doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have avx core. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mHence, no-avx core with worse performance will be imported.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf you like, you could \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mreinstall paddlepaddle by \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpython -m pip install --force-reinstall paddlepaddle-gpu[==version]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mto get better performance.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: libssl.so.1.1: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import paddle\n",
    "import random\n",
    "import traceback\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn.initializer as init\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from paddle import ParamAttr\n",
    "from paddle.optimizer.lr import *\n",
    "from collections import OrderedDict\n",
    "from collections.abc import Sequence\n",
    "from paddle.regularizer import L2Decay\n",
    "from paddle.nn import (Conv2D, BatchNorm2D, Linear, Dropout, MaxPool2D,\n",
    "                       AvgPool2D, AdaptiveAvgPool2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**将视频解码为帧**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VideoDecoder(object):\n",
    "    \"\"\"\n",
    "    Decode mp4 file to frames.\n",
    "    Args:\n",
    "        filepath: the file path of mp4 file\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"\n",
    "        Perform mp4 decode operations.\n",
    "        return:\n",
    "            List where each item is a numpy array after decoder.\n",
    "        \"\"\"\n",
    "        file_path = results['filename']\n",
    "        results['format'] = 'video'\n",
    "\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        videolen = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sampledFrames = []\n",
    "        for i in range(videolen):\n",
    "            ret, frame = cap.read()\n",
    "            # maybe first frame is empty\n",
    "            if ret == False:\n",
    "                continue\n",
    "            img = frame[:, :, ::-1]\n",
    "            sampledFrames.append(img)\n",
    "        results['frames'] = sampledFrames\n",
    "        results['frames_len'] = len(sampledFrames)    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**帧采样**\n",
    "\n",
    "对一段视频进行分段采样，大致流程为：\n",
    "\n",
    "1. 对视频进行分段；\n",
    "2. 从每段视频随机选取一个起始位置；\n",
    "3. 从选取的起始位置采集连续的k帧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sampler(object):\n",
    "    \"\"\"\n",
    "    Sample frames id.\n",
    "    NOTE: Use PIL to read image here, has diff with CV2\n",
    "    Args:\n",
    "        num_seg(int): number of segments.\n",
    "        seg_len(int): number of sampled frames in each segment.\n",
    "        valid_mode(bool): True or False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_seg,\n",
    "                 seg_len,\n",
    "                 valid_mode=False):\n",
    "        self.num_seg = num_seg\n",
    "        self.seg_len = seg_len\n",
    "        self.valid_mode = valid_mode\n",
    "\n",
    "    def _get(self, frames_idx, results):\n",
    "        data_format = results['format']\n",
    "        if data_format == 'video':\n",
    "            frames = np.array(results['frames'])\n",
    "            imgs = []\n",
    "            for idx in frames_idx:\n",
    "                imgbuf = frames[idx]\n",
    "                img = Image.fromarray(imgbuf, mode='RGB')\n",
    "                imgs.append(img)\n",
    "            results['imgs'] = imgs\n",
    "        return results      \n",
    "\n",
    "    def __call__(self, results):\n",
    "        frames_len = int(results['frames_len'])\n",
    "        average_dur = int(frames_len / self.num_seg)\n",
    "        frames_idx = []\n",
    "        \n",
    "        for i in range(self.num_seg):\n",
    "            idx = 0\n",
    "            if not self.valid_mode:\n",
    "                if average_dur >= self.seg_len:\n",
    "                    # !!!!!!!\n",
    "                    idx = random.randint(0, average_dur - self.seg_len)\n",
    "                    #idx = 0\n",
    "                    idx += i * average_dur\n",
    "                elif average_dur >= 1:\n",
    "                    idx += i * average_dur\n",
    "                else:\n",
    "                    idx = i\n",
    "            else:\n",
    "                if average_dur >= self.seg_len:\n",
    "                    idx = (average_dur - 1) // 2\n",
    "                    idx += i * average_dur\n",
    "                elif average_dur >= 1:\n",
    "                    idx += i * average_dur\n",
    "                else:\n",
    "                    idx = i\n",
    "            for jj in range(idx, idx + self.seg_len):\n",
    "                if results['format'] == 'video':\n",
    "                    frames_idx.append(int(jj % frames_len))\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "        return self._get(frames_idx, results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**图片尺度化**\n",
    "\n",
    "图片尺度化的目的是将图片中短边 resize 到固定的尺寸，图片中的长边按照等比例进行缩放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Scale(object):\n",
    "    \"\"\"\n",
    "    Scale images.\n",
    "    Args:\n",
    "        short_size(float | int): Short size of an image will be scaled to the short_size.\n",
    "        fixed_ratio(bool): Set whether to zoom according to a fixed ratio. default: True\n",
    "        do_round(bool): Whether to round up when calculating the zoom ratio. default: False\n",
    "        backend(str): Choose pillow or cv2 as the graphics processing backend. default: 'pillow'\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 short_size,\n",
    "                 fixed_ratio=True,\n",
    "                 do_round=False):\n",
    "        self.short_size = short_size\n",
    "        self.fixed_ratio = fixed_ratio\n",
    "        self.do_round = do_round\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"\n",
    "        Performs resize operations.\n",
    "        Args:\n",
    "            imgs (Sequence[PIL.Image]): List where each item is a PIL.Image.\n",
    "            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n",
    "        return:\n",
    "            resized_imgs: List where each item is a PIL.Image after scaling.\n",
    "        \"\"\"\n",
    "        imgs = results['imgs']\n",
    "        resized_imgs = []\n",
    "        for i in range(len(imgs)):\n",
    "            img = imgs[i]\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.short_size) or (h <= w\n",
    "                                                     and h == self.short_size):\n",
    "                resized_imgs.append(img)\n",
    "                continue\n",
    "            if w < h:\n",
    "                ow = self.short_size\n",
    "                if self.fixed_ratio:\n",
    "                    oh = int(self.short_size * 4.0 / 3.0)\n",
    "                else:\n",
    "                    oh = int(round(h * self.short_size /\n",
    "                                   w)) if self.do_round else int(\n",
    "                                       h * self.short_size / w)\n",
    "            else:\n",
    "                oh = self.short_size\n",
    "                if self.fixed_ratio:\n",
    "                    ow = int(self.short_size * 4.0 / 3.0)\n",
    "                else:\n",
    "                    ow = int(round(w * self.short_size /\n",
    "                                   h)) if self.do_round else int(\n",
    "                                       w * self.short_size / h)\n",
    "            resized_imgs.append(img.resize((ow, oh), Image.BILINEAR))\n",
    "        results['imgs'] = resized_imgs\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**多尺度剪裁**\n",
    "\n",
    "从多个尺度中随机选择一个裁剪尺度，并计算具体裁剪起始位置以及宽和高，之后从原图中裁剪出随机的固定区域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiScaleCrop(object):\n",
    "    \"\"\"\n",
    "    Random crop images in with multiscale sizes\n",
    "    Args:\n",
    "        target_size(int): Random crop a square with the target_size from an image.\n",
    "        scales(int): List of candidate cropping scales.\n",
    "        max_distort(int): Maximum allowable deformation combination distance.\n",
    "        fix_crop(int): Whether to fix the cutting start point.\n",
    "        allow_duplication(int): Whether to allow duplicate candidate crop starting points.\n",
    "        more_fix_crop(int): Whether to allow more cutting starting points.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            target_size,  # NOTE: named target size now, but still pass short size in it!\n",
    "            scales=None,\n",
    "            max_distort=1,\n",
    "            fix_crop=True,\n",
    "            allow_duplication=False,\n",
    "            more_fix_crop=True):\n",
    "\n",
    "        self.target_size = target_size\n",
    "        self.scales = scales if scales else [1, .875, .75, .66]\n",
    "        self.max_distort = max_distort\n",
    "        self.fix_crop = fix_crop\n",
    "        self.allow_duplication = allow_duplication\n",
    "        self.more_fix_crop = more_fix_crop\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"\n",
    "        Performs MultiScaleCrop operations.\n",
    "        Args:\n",
    "            imgs: List where wach item is a PIL.Image.\n",
    "            XXX:\n",
    "        results:\n",
    "        \"\"\"\n",
    "        imgs = results['imgs']\n",
    "\n",
    "        input_size = [self.target_size, self.target_size]\n",
    "\n",
    "        im_size = imgs[0].size\n",
    "\n",
    "        # get random crop offset\n",
    "        def _sample_crop_size(im_size):\n",
    "            image_w, image_h = im_size[0], im_size[1]\n",
    "\n",
    "            base_size = min(image_w, image_h)\n",
    "            crop_sizes = [int(base_size * x) for x in self.scales]\n",
    "            crop_h = [\n",
    "                input_size[1] if abs(x - input_size[1]) < 3 else x\n",
    "                for x in crop_sizes\n",
    "            ]\n",
    "            crop_w = [\n",
    "                input_size[0] if abs(x - input_size[0]) < 3 else x\n",
    "                for x in crop_sizes\n",
    "            ]\n",
    "\n",
    "            pairs = []\n",
    "            for i, h in enumerate(crop_h):\n",
    "                for j, w in enumerate(crop_w):\n",
    "                    if abs(i - j) <= self.max_distort:\n",
    "                        pairs.append((w, h))\n",
    "            # !!!!!!!\n",
    "            crop_pair = random.choice(pairs)\n",
    "            #crop_pair = pairs[0]\n",
    "\n",
    "            if not self.fix_crop:\n",
    "                # !!!!!!!\n",
    "                w_offset = random.randint(0, image_w - crop_pair[0])\n",
    "                h_offset = random.randint(0, image_h - crop_pair[1])\n",
    "                #w_offset = 0\n",
    "                #h_offset = 0\n",
    "\n",
    "            else:\n",
    "                w_step = (image_w - crop_pair[0]) / 4\n",
    "                h_step = (image_h - crop_pair[1]) / 4\n",
    "\n",
    "                ret = list()\n",
    "                ret.append((0, 0))  # upper left\n",
    "                if self.allow_duplication or w_step != 0:\n",
    "                    ret.append((4 * w_step, 0))  # upper right\n",
    "                if self.allow_duplication or h_step != 0:\n",
    "                    ret.append((0, 4 * h_step))  # lower left\n",
    "                if self.allow_duplication or (h_step != 0 and w_step != 0):\n",
    "                    ret.append((4 * w_step, 4 * h_step))  # lower right\n",
    "                if self.allow_duplication or (h_step != 0 or w_step != 0):\n",
    "                    ret.append((2 * w_step, 2 * h_step))  # center\n",
    "\n",
    "                if self.more_fix_crop:\n",
    "                    ret.append((0, 2 * h_step))  # center left\n",
    "                    ret.append((4 * w_step, 2 * h_step))  # center right\n",
    "                    ret.append((2 * w_step, 4 * h_step))  # lower center\n",
    "                    ret.append((2 * w_step, 0 * h_step))  # upper center\n",
    "\n",
    "                    ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n",
    "                    ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n",
    "                    ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n",
    "                    ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n",
    "                \n",
    "                # !!!!!!!\n",
    "                #w_offset, h_offset = ret[0]\n",
    "                w_offset, h_offset = random.choice(ret)\n",
    "\n",
    "            return crop_pair[0], crop_pair[1], w_offset, h_offset\n",
    "\n",
    "        crop_w, crop_h, offset_w, offset_h = _sample_crop_size(im_size)\n",
    "        crop_img_group = [\n",
    "            img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h))\n",
    "            for img in imgs\n",
    "        ]\n",
    "        ret_img_group = [\n",
    "            img.resize((input_size[0], input_size[1]), Image.BILINEAR)\n",
    "            for img in crop_img_group\n",
    "        ]\n",
    "        results['imgs'] = ret_img_group\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**随机翻转**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomFlip(object):\n",
    "    \"\"\"\n",
    "    Random Flip images.\n",
    "    Args:\n",
    "        p(float): Random flip images with the probability p.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"\n",
    "        Performs random flip operations.\n",
    "        Args:\n",
    "            imgs: List where each item is a PIL.Image.\n",
    "            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n",
    "        return:\n",
    "            flip_imgs: List where each item is a PIL.Image after random flip.\n",
    "        \"\"\"\n",
    "        imgs = results['imgs']\n",
    "        # !!!!!!\n",
    "        v = random.random()\n",
    "        #v = 0\n",
    "        if v < self.p:\n",
    "            results['imgs'] = [\n",
    "                img.transpose(Image.FLIP_LEFT_RIGHT) for img in imgs\n",
    "            ]\n",
    "        else:\n",
    "            results['imgs'] = imgs\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**数据格式转换**\n",
    "\n",
    "将数据格式由PIL.Image转换为Numpy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Image2Array(object):\n",
    "    \"\"\"\n",
    "    transfer PIL.Image to Numpy array and transpose dimensions from 'dhwc' to 'dchw'.\n",
    "    Args:\n",
    "        transpose: whether to transpose or not, default True, False for slowfast.\n",
    "    \"\"\"\n",
    "    def __init__(self, transpose=True):\n",
    "        self.transpose = transpose\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"\n",
    "        Performs Image to NumpyArray operations.\n",
    "        Args:\n",
    "            imgs: List where each item is a PIL.Image.\n",
    "            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n",
    "        return:\n",
    "            np_imgs: Numpy array.\n",
    "        \"\"\"\n",
    "        imgs = results['imgs']\n",
    "        np_imgs = (np.stack(imgs)).astype('float32')\n",
    "        if self.transpose:\n",
    "            np_imgs = np_imgs.transpose(0, 3, 1, 2)  # tchw\n",
    "        results['imgs'] = np_imgs\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**归一化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Normalization(object):\n",
    "    \"\"\"\n",
    "    Normalization.\n",
    "    Args:\n",
    "        mean(Sequence[float]): mean values of different channels.\n",
    "        std(Sequence[float]): std values of different channels.\n",
    "        tensor_shape(list): size of mean, default [3,1,1]. For slowfast, [1,1,1,3]\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std, tensor_shape=[3, 1, 1]):\n",
    "        if not isinstance(mean, Sequence):\n",
    "            raise TypeError(\n",
    "                f'Mean must be list, tuple or np.ndarray, but got {type(mean)}')\n",
    "        if not isinstance(std, Sequence):\n",
    "            raise TypeError(\n",
    "                f'Std must be list, tuple or np.ndarray, but got {type(std)}')\n",
    "        self.mean = np.array(mean).reshape(tensor_shape).astype(np.float32)\n",
    "        self.std = np.array(std).reshape(tensor_shape).astype(np.float32)\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"\n",
    "        Performs normalization operations.\n",
    "        Args:\n",
    "            imgs: Numpy array.\n",
    "        return:\n",
    "            np_imgs: Numpy array after normalization.\n",
    "        \"\"\"\n",
    "        imgs = results['imgs']\n",
    "        norm_imgs = imgs / 255.0\n",
    "        norm_imgs -= self.mean\n",
    "        norm_imgs /= self.std\n",
    "        results['imgs'] = norm_imgs\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**随机剪裁**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    \"\"\"\n",
    "    Random crop images.\n",
    "    Args:\n",
    "        target_size(int): Random crop a square with the target_size from an image.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"\n",
    "        Performs random crop operations.\n",
    "        Args:\n",
    "            imgs: List where each item is a PIL.Image.\n",
    "            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n",
    "        return:\n",
    "            crop_imgs: List where each item is a PIL.Image after random crop.\n",
    "        \"\"\"\n",
    "        imgs = results['imgs']\n",
    "        if 'backend' in results and results['backend'] == 'pyav':  # [c,t,h,w]\n",
    "            h, w = imgs.shape[2:]\n",
    "        else:\n",
    "            w, h = imgs[0].size\n",
    "        th, tw = self.target_size, self.target_size\n",
    "\n",
    "        assert (w >= self.target_size) and (h >= self.target_size), \\\n",
    "            \"image width({}) and height({}) should be larger than crop size\".format(\n",
    "                w, h, self.target_size)\n",
    "\n",
    "        crop_images = []\n",
    "        if 'backend' in results and results['backend'] == 'pyav':\n",
    "            x1 = np.random.randint(0, w - tw)\n",
    "            y1 = np.random.randint(0, h - th)\n",
    "            crop_images = imgs[:, :, y1:y1 + th, x1:x1 + tw]  # [C, T, th, tw]\n",
    "        else:\n",
    "            x1 = random.randint(0, w - tw)\n",
    "            y1 = random.randint(0, h - th)\n",
    "            for img in imgs:\n",
    "                if w == tw and h == th:\n",
    "                    crop_images.append(img)\n",
    "                else:\n",
    "                    crop_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
    "        results['imgs'] = crop_images\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**中心剪裁**\n",
    "\n",
    "中心裁剪与随机裁剪类似，具体的差异在于选取裁剪起始点的方法不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CenterCrop(object):\n",
    "    \"\"\"\n",
    "    Center crop images.\n",
    "    Args:\n",
    "        target_size(int): Center crop a square with the target_size from an image.\n",
    "        do_round(bool): Whether to round up the coordinates of the upper left corner of the cropping area. default: True\n",
    "    \"\"\"\n",
    "    def __init__(self, target_size, do_round=True):\n",
    "        self.target_size = target_size\n",
    "        self.do_round = do_round\n",
    "\n",
    "    def __call__(self, results):\n",
    "        \"\"\"\n",
    "        Performs Center crop operations.\n",
    "        Args:\n",
    "            imgs: List where each item is a PIL.Image.\n",
    "            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n",
    "        return:\n",
    "            ccrop_imgs: List where each item is a PIL.Image after Center crop.\n",
    "        \"\"\"\n",
    "        imgs = results['imgs']\n",
    "        ccrop_imgs = []\n",
    "        for img in imgs:\n",
    "            w, h = img.size\n",
    "            th, tw = self.target_size, self.target_size\n",
    "            assert (w >= self.target_size) and (h >= self.target_size), \\\n",
    "                \"image width({}) and height({}) should be larger than crop size\".format(\n",
    "                    w, h, self.target_size)\n",
    "            x1 = int(round((w - tw) / 2.0)) if self.do_round else (w - tw) // 2\n",
    "            y1 = int(round((h - th) / 2.0)) if self.do_round else (h - th) // 2\n",
    "            ccrop_imgs.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
    "        results['imgs'] = ccrop_imgs\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**数据增强方式Mixup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Mixup(object):\n",
    "    \"\"\"\n",
    "    Mixup operator.\n",
    "    Args:\n",
    "        alpha(float): alpha value.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.2):\n",
    "        assert alpha > 0., \\\n",
    "                'parameter alpha[%f] should > 0.0' % (alpha)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs, labels = list(zip(*batch))\n",
    "        imgs = np.array(imgs)\n",
    "        labels = np.array(labels)\n",
    "        bs = len(batch)\n",
    "        idx = np.random.permutation(bs)\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        lams = np.array([lam] * bs, dtype=np.float32)\n",
    "        imgs = lam * imgs + (1 - lam) * imgs[idx]\n",
    "        return list(zip(imgs, labels, labels[idx], lams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1.4 数据预处理模块组合\n",
    "\n",
    "为方便对数据进行组合处理，将训练模式和验证模式下的数据预处理模块进行封装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    \"\"\"\n",
    "    Composes several pipelines(include decode func, sample func, and transforms) together.\n",
    "\n",
    "    Note: To deal with ```list``` type cfg temporaray, like:\n",
    "\n",
    "        transform:\n",
    "            - Crop: # A list\n",
    "                attribute: 10\n",
    "            - Resize: # A list\n",
    "                attribute: 20\n",
    "\n",
    "    every key of list will pass as the key name to build a module.\n",
    "    XXX: will be improved in the future.\n",
    "\n",
    "    Args:\n",
    "        pipelines (list): List of transforms to compose.\n",
    "    Returns:\n",
    "        A compose object which is callable, __call__ for this Compose\n",
    "        object will call each given :attr:`transforms` sequencely.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_mode=False):\n",
    "        # assert isinstance(pipelines, Sequence)\n",
    "        self.pipelines = list()\n",
    "        self.pipelines.append(VideoDecoder())\n",
    "        if train_mode:\n",
    "            self.pipelines.append(Sampler(num_seg=8, seg_len=1, valid_mode=False))\n",
    "            #self.pipelines.append(MultiScaleCrop(target_size=224, allow_duplication=True))\n",
    "            self.pipelines.append(MultiScaleCrop(target_size=256))\n",
    "            self.pipelines.append(RandomCrop(target_size=224))\n",
    "            self.pipelines.append(RandomFlip())\n",
    "        else:\n",
    "            self.pipelines.append(Sampler(num_seg=8, seg_len=1, valid_mode=True))\n",
    "            self.pipelines.append(Scale(short_size=256, fixed_ratio=False))\n",
    "            self.pipelines.append(CenterCrop(target_size=224))\n",
    "        self.pipelines.append(Image2Array())\n",
    "        self.pipelines.append(Normalization(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # 将传入的 data 依次经过 pipelines 中对象处理\n",
    "        for p in self.pipelines:\n",
    "            try:\n",
    "                data = p(data)\n",
    "            except Exception as e:\n",
    "                stack_info = traceback.format_exc()\n",
    "                print(\"fail to perform transform [{}] with error: \"\n",
    "                            \"{} and stack:\\n{}\".format(p, e, str(stack_info)))\n",
    "                raise e\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1.5 数据读取\n",
    "接下来我们通过继承paddle的[Dataset API](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/Dataset_cn.html)来构建一个数据读取器，方便每次从数据中获取一个样本和对应的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VideoDataset(paddle.io.Dataset):\n",
    "    \"\"\"Video dataset for action recognition\n",
    "       The dataset loads raw videos and apply specified transforms on them.\n",
    "       The index file is a file with multiple lines, and each line indicates\n",
    "       a sample video with the filepath and label, which are split with a whitesapce.\n",
    "       Example of a inde file:\n",
    "       .. code-block:: txt\n",
    "           path/000.mp4 1\n",
    "           path/001.mp4 1\n",
    "           path/002.mp4 2\n",
    "           path/003.mp4 2\n",
    "       Args:\n",
    "           file_path(str): Path to the index file.\n",
    "           pipeline(XXX): A sequence of data transforms.\n",
    "           **kwargs: Keyword arguments for ```BaseDataset```.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, pipeline, num_retries=5, suffix='', test_mode=False):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.pipeline = pipeline\n",
    "        self.num_retries = num_retries\n",
    "        self.suffix = suffix\n",
    "        self.info = self.load_file()\n",
    "        self.test_mode = test_mode\n",
    "        \n",
    "    def load_file(self):\n",
    "        \"\"\"Load index file to get video information.\"\"\"\n",
    "        info = []\n",
    "        with open(self.file_path, 'r') as fin:\n",
    "            for line in fin:\n",
    "                line_split = line.strip().split()\n",
    "                filename, labels = line_split\n",
    "                filename = filename + self.suffix\n",
    "                info.append(dict(filename=filename, labels=int(labels)))\n",
    "        return info\n",
    "\n",
    "    def prepare_train(self, idx):\n",
    "        \"\"\"TRAIN & VALID. Prepare the data for training/valid given the index.\"\"\"\n",
    "        #Try to catch Exception caused by reading corrupted video file\n",
    "        for ir in range(self.num_retries):\n",
    "            try:\n",
    "                results = copy.deepcopy(self.info[idx])\n",
    "                results = self.pipeline(results)\n",
    "            except Exception as e:\n",
    "                if ir < self.num_retries - 1:\n",
    "                    print(\n",
    "                        \"Error when loading {}, have {} trys, will try again\".\n",
    "                        format(results['filename'], ir))\n",
    "                idx = random.randint(0, len(self.info) - 1)\n",
    "                continue\n",
    "            return results['imgs'], np.array([results['labels']])\n",
    "\n",
    "    def prepare_test(self, idx):\n",
    "        \"\"\"TEST. Prepare the data for test given the index.\"\"\"\n",
    "        #Try to catch Exception caused by reading corrupted video file\n",
    "        for ir in range(self.num_retries):\n",
    "            try:\n",
    "                results = copy.deepcopy(self.info[idx])\n",
    "                results = self.pipeline(results)\n",
    "            except Exception as e:\n",
    "                if ir < self.num_retries - 1:\n",
    "                    print(\n",
    "                        \"Error when loading {}, have {} trys, will try again\".\n",
    "                        format(results['filename'], ir))\n",
    "                idx = random.randint(0, len(self.info) - 1)\n",
    "                continue\n",
    "            return results['imgs'], np.array([results['labels']])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"get the size of the dataset.\"\"\"\n",
    "        return len(self.info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get the sample for either training or testing given index\"\"\"\n",
    "        if self.test_mode:\n",
    "            return self.prepare_test(idx)\n",
    "        else:\n",
    "            return self.prepare_train(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataloader(dataset,\n",
    "                     batch_size,\n",
    "                     num_workers,\n",
    "                     train_mode,\n",
    "                     places,\n",
    "                     shuffle=True,\n",
    "                     drop_last=True,\n",
    "                     collate_fn_cfg=None):\n",
    "    \"\"\"Build Paddle Dataloader.\n",
    "    XXX explain how the dataloader work!\n",
    "    Args:\n",
    "        dataset (paddle.dataset): A PaddlePaddle dataset object.\n",
    "        batch_size (int): batch size on single card.\n",
    "        num_worker (int): num_worker\n",
    "        shuffle(bool): whether to shuffle the data at every epoch.\n",
    "    \"\"\"\n",
    "    sampler = paddle.io.DistributedBatchSampler(dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=shuffle,\n",
    "                                        drop_last=drop_last)\n",
    "\n",
    "    #NOTE(shipping): when switch the mix operator on, such as: mixup, cutmix.\n",
    "    # batch like: [[img, label, attibute, ...], [imgs, label, attribute, ...], ...] will recollate to:\n",
    "    # [[img, img, ...], [label, label, ...], [attribute, attribute, ...], ...] as using numpy.transpose.\n",
    "\n",
    "    def mix_collate_fn(batch):\n",
    "        pipeline = collate_fn_cfg\n",
    "        batch = pipeline(batch)\n",
    "        slots = []\n",
    "        for items in batch:\n",
    "            for i, item in enumerate(items):\n",
    "                if len(slots) < len(items):\n",
    "                    slots.append([item])\n",
    "                else:\n",
    "                    slots[i].append(item)\n",
    "        return [np.stack(slot, axis=0) for slot in slots]\n",
    "\n",
    "    data_loader = paddle.io.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=sampler,\n",
    "        places=places,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=mix_collate_fn if collate_fn_cfg is not None else None,\n",
    "        return_list=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 模型构建\n",
    "### 2.2.1 PP-TSM 简介\n",
    "#### 背景\n",
    "\n",
    "相较于图像，视频具有额外的时间维度信息，因此如何更好的利用视频中的时序信息是视频研究的重点。目前常用的方法有三类：  \n",
    "1. 使用RNN对视频特征进行时序建模，如AttentionLSTM模型。这类模型的输入是视频特征，而不是原始视频，因此往往用作后处理模块。 \n",
    "2. 使用3D网络提取时序信息。如SlowFast模型，使用Slow和Fast两个网络分支分别捕获视频中的表观信息和运动信息，该模型在视频分类任务上取得了SOTA的效果，同时是AVA 视频检测挑战赛的冠军模型。  \n",
    "3. 使用2D网络提取时序信息，如经典的TSN和TSM模型。相较于TSN模型，TSM模型使用时序位移模块对时序信息建模，在不增加计算量的前提下提升网络的精度，非常适合工业落地。  \n",
    "\n",
    "PP-TSM模型在基本不增加计算量的前提下，使用Kinetics-400数据集训练的精度可以提升到76.16%，超过同等Backbone下3D模型SlowFast，且推理速度快4.5倍，具有显著的性能优势。\n",
    "\n",
    "#### 精度优化Tricks详解\n",
    "1. 数据增强Video Mix-up\n",
    "\n",
    "Mix-up是图像领域常用的数据增广方法，它将两幅图像以一定的权值叠加构成新的输入图像。对于视频Mix-up，即是将两个视频以一定的权值叠加构成新的输入视频。相较于图像，视频由于多了时间维度，混合的方式可以有更多的选择。实验中，我们对每个视频，首先抽取固定数量的帧，并给每一帧赋予相同的权重，然后与另一个视频叠加作为新的输入视频。结果表明，这种Mix-up方式能有效提升网络在时空上的抗干扰能力。\n",
    "\n",
    "2. 更优的网络结构\n",
    "\n",
    "Better Backbone：骨干网络可以说是一个模型的基础，一个优秀的骨干网络会给模型的性能带来极大的提升。针对TSM，飞桨研发人员使用更加优异的ResNet50_vd作为模型的骨干网络，在保持原有参数量的同时提升了模型精度。ResNet50_vd是指拥有50个卷积层的ResNet-D网络。如下图所示，ResNet系列网络在被提出后经过了B、C、D三个版本的改进。ResNet-B将Path A中1*1卷积的stride由2改为1，避免了信息丢失；ResNet-C将第一个7*7的卷积核调整为3个3*3卷积核，减少计算量的同时增加了网络非线性；ResNet-D进一步将Path B中1*1卷积的stride由2改为1，并添加了平均池化层，保留了更多的信息。\n",
    "\n",
    "\n",
    "Feature aggregation：对TSM模型，在骨干网络提取特征后，还需要使用分类器做特征分类。实验表明，在特征平均之后分类，可以减少frame-level特征的干扰，获得更高的精度。假设输入视频抽取的帧数为N，则经过骨干网络后，可以得到N个frame-level特征。分类器有两种实现方式：第一种是先对N个帧级特征进行平均，得到视频级特征后，再用全连接层进行分类；另一种方式是先接全连接层，得到N个权重后进行平均。飞桨开发人员经过大量实验验证发现，采用第1种方式有更好的精度收益。\n",
    "\n",
    "3. 更稳定的训练策略\n",
    "\n",
    "Cosine decay LR：在使用梯度下降算法优化目标函数时，我们使用余弦退火策略调整学习率。同时使用Warm-up策略，在模型训练之初选用较小的学习率，训练一段时间之后再使用预设的学习率训练，这使得收敛过程更加快速平滑。\n",
    "\n",
    "Scale fc learning rate：在训练过程中，我们给全连接层设置的学习率为其它层的5倍。实验结果表明，通过给分类器层设置更大的学习率，有助于网络更好的学习收敛，提升模型精度。\n",
    "\n",
    "4. Label smooth\n",
    "\n",
    "标签平滑是一种对分类器层进行正则化的机制，通过在真实的分类标签one-hot编码中真实类别的1上减去一个小量，非真实标签的0上加上一个小量，将硬标签变成一个软标签，达到正则化的作用，防止过拟合，提升模型泛化能力。\n",
    "\n",
    "5. Precise BN\n",
    "\n",
    "假定训练数据的分布和测试数据的分布是一致的，对于Batch Normalization层，通常在训练过程中会计算滑动均值和滑动方差，供测试时使用。但滑动均值并不等于真实的均值，因此测试时的精度仍会受到一定影响。为了获取更加精确的均值和方差供BN层在测试时使用，在实验中，我们会在网络训练完一个Epoch后，固定住网络中的参数不动，然后将训练数据输入网络做前向计算，保存下来每个step的均值和方差，最终得到所有训练样本精确的均值和方差，提升测试精度。\n",
    "\n",
    "6. 知识蒸馏方案：Two Stages Knowledge Distillation\n",
    "\n",
    "我们使用两阶段知识蒸馏方案提升模型精度。第一阶段使用半监督标签知识蒸馏方法对图像分类模型进行蒸馏，以获得具有更好分类效果的pretrain模型。第二阶段使用更高精度的视频分类模型作为教师模型进行蒸馏，以进一步提升模型精度。实验中，将以ResNet152为Backbone的CSN模型作为第二阶段蒸馏的教师模型，在uniform和dense评估策略下，精度均可以提升大约0.6个点。最终PP-TSM精度达到76.16，超过同等Backbone下的SlowFast模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2.2 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConvBNLayer(nn.Layer):\n",
    "    \"\"\"Conv2D and BatchNorm2D layer.\n",
    "    Args:\n",
    "        in_channels (int): Number of channels for the input.\n",
    "        out_channels (int): Number of channels for the output.\n",
    "        kernel_size (int): Kernel size.\n",
    "        stride (int): Stride in the Conv2D layer. Default: 1.\n",
    "        groups (int): Groups in the Conv2D, Default: 1.\n",
    "        is_tweaks_mode (bool): switch for tweaks. Default: False.\n",
    "        act (str): Indicate activation after BatchNorm2D layer.\n",
    "        name (str): the name of an instance of ConvBNLayer.\n",
    "    Note: weight and bias initialization include initialize values and name the restored parameters, values initialization are explicit declared in the ```init_weights``` method.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 groups=1,\n",
    "                 is_tweaks_mode=False,\n",
    "                 act=None,\n",
    "                 name=None):\n",
    "        super(ConvBNLayer, self).__init__()\n",
    "        self.is_tweaks_mode = is_tweaks_mode\n",
    "        #ResNet-D 1/2:add a 2×2 average pooling layer with a stride of 2 before the convolution,\n",
    "        #             whose stride is changed to 1, works well in practice.\n",
    "        self._pool2d_avg = AvgPool2D(kernel_size=2,\n",
    "                                     stride=2,\n",
    "                                     padding=0,\n",
    "                                     ceil_mode=True)\n",
    "\n",
    "        self._conv = Conv2D(in_channels=in_channels,\n",
    "                            out_channels=out_channels,\n",
    "                            kernel_size=kernel_size,\n",
    "                            stride=stride,\n",
    "                            padding=(kernel_size - 1) // 2,\n",
    "                            groups=groups,\n",
    "                            weight_attr=ParamAttr(name=name + \"_weights\"),\n",
    "                            bias_attr=False)\n",
    "        if name == \"conv1\":\n",
    "            bn_name = \"bn_\" + name\n",
    "        else:\n",
    "            bn_name = \"bn\" + name[3:]\n",
    "\n",
    "        self._act = act\n",
    "\n",
    "        self._batch_norm = BatchNorm2D(\n",
    "            out_channels,\n",
    "            weight_attr=ParamAttr(name=bn_name + \"_scale\",\n",
    "                                  regularizer=L2Decay(0.0)),\n",
    "            bias_attr=ParamAttr(bn_name + \"_offset\", regularizer=L2Decay(0.0)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.is_tweaks_mode:\n",
    "            inputs = self._pool2d_avg(inputs)\n",
    "        y = self._conv(inputs)\n",
    "        y = self._batch_norm(y)\n",
    "        if self._act:\n",
    "            y = getattr(paddle.nn.functional, self._act)(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 stride,\n",
    "                 shortcut=True,\n",
    "                 if_first=False,\n",
    "                 num_seg=8,\n",
    "                 name=None):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        self.conv0 = ConvBNLayer(in_channels=in_channels,\n",
    "                                 out_channels=out_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 act=\"leaky_relu\",\n",
    "                                 name=name + \"_branch2a\")\n",
    "        self.conv1 = ConvBNLayer(in_channels=out_channels,\n",
    "                                 out_channels=out_channels,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=stride,\n",
    "                                 act=\"leaky_relu\",\n",
    "                                 name=name + \"_branch2b\")\n",
    "\n",
    "        self.conv2 = ConvBNLayer(in_channels=out_channels,\n",
    "                                 out_channels=out_channels * 4,\n",
    "                                 kernel_size=1,\n",
    "                                 act=None,\n",
    "                                 name=name + \"_branch2c\")\n",
    "\n",
    "        if not shortcut:\n",
    "            self.short = ConvBNLayer(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels * 4,\n",
    "                kernel_size=1,\n",
    "                stride=\n",
    "                1,  #ResNet-D 2/2:add a 2×2 average pooling layer with a stride of 2 before the convolution,\n",
    "                #             whose stride is changed to 1, works well in practice.\n",
    "                is_tweaks_mode=False if if_first else True,\n",
    "                name=name + \"_branch1\")\n",
    "\n",
    "        self.shortcut = shortcut\n",
    "        self.num_seg = num_seg\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        shifts = paddle.fluid.layers.temporal_shift(inputs, self.num_seg,\n",
    "                                                    1.0 / self.num_seg)\n",
    "        y = self.conv0(shifts)\n",
    "        conv1 = self.conv1(y)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        if self.shortcut:\n",
    "            short = inputs\n",
    "        else:\n",
    "            short = self.short(inputs)\n",
    "        y = paddle.add(x=short, y=conv2)\n",
    "        return F.leaky_relu(y)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 stride,\n",
    "                 shortcut=True,\n",
    "                 name=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.conv0 = ConvBNLayer(in_channels=in_channels,\n",
    "                                 out_channels=out_channels,\n",
    "                                 filter_size=3,\n",
    "                                 stride=stride,\n",
    "                                 act=\"leaky_relu\",\n",
    "                                 name=name + \"_branch2a\")\n",
    "        self.conv1 = ConvBNLayer(in_channels=out_channels,\n",
    "                                 out_channels=out_channels,\n",
    "                                 filter_size=3,\n",
    "                                 act=None,\n",
    "                                 name=name + \"_branch2b\")\n",
    "\n",
    "        if not shortcut:\n",
    "            self.short = ConvBNLayer(in_channels=in_channels,\n",
    "                                     out_channels=out_channels,\n",
    "                                     filter_size=1,\n",
    "                                     stride=stride,\n",
    "                                     name=name + \"_branch1\")\n",
    "\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        y = self.conv0(inputs)\n",
    "        conv1 = self.conv1(y)\n",
    "\n",
    "        if self.shortcut:\n",
    "            short = inputs\n",
    "        else:\n",
    "            short = self.short(inputs)\n",
    "        y = paddle.add(short, conv1)\n",
    "        y = F.leaky_relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResNetTweaksTSM(nn.Layer):\n",
    "    \"\"\"ResNet TSM backbone.\n",
    "    Args:\n",
    "        depth (int): Depth of resnet model.\n",
    "        pretrained (str): pretrained model. Default: None.\n",
    "    \"\"\"\n",
    "    def __init__(self, depth, num_seg=8, pretrained=None):\n",
    "        super(ResNetTweaksTSM, self).__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.layers = depth\n",
    "        self.num_seg = num_seg\n",
    "\n",
    "        supported_layers = [18, 34, 50, 101, 152]\n",
    "        assert self.layers in supported_layers, \\\n",
    "            \"supported layers are {} but input layer is {}\".format(\n",
    "                supported_layers, self.layers)\n",
    "\n",
    "        if self.layers == 18:\n",
    "            depth = [2, 2, 2, 2]\n",
    "        elif self.layers == 34 or self.layers == 50:\n",
    "            depth = [3, 4, 6, 3]\n",
    "        elif self.layers == 101:\n",
    "            depth = [3, 4, 23, 3]\n",
    "        elif self.layers == 152:\n",
    "            depth = [3, 8, 36, 3]\n",
    "\n",
    "        in_channels = 64\n",
    "        out_channels = [64, 128, 256, 512]\n",
    "\n",
    "        #ResNet-C: use three 3x3 conv, replace, one 7x7 conv\n",
    "        self.conv1_1 = ConvBNLayer(in_channels=3,\n",
    "                                   out_channels=32,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=2,\n",
    "                                   act='leaky_relu',\n",
    "                                   name=\"conv1_1\")\n",
    "        self.conv1_2 = ConvBNLayer(in_channels=32,\n",
    "                                   out_channels=32,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   act='leaky_relu',\n",
    "                                   name=\"conv1_2\")\n",
    "        self.conv1_3 = ConvBNLayer(in_channels=32,\n",
    "                                   out_channels=64,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   act='leaky_relu',\n",
    "                                   name=\"conv1_3\")\n",
    "        self.pool2D_max = MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.block_list = []\n",
    "        if self.layers >= 50:\n",
    "            for block in range(len(depth)):\n",
    "                shortcut = False\n",
    "                for i in range(depth[block]):\n",
    "                    if self.layers in [101, 152] and block == 2:\n",
    "                        if i == 0:\n",
    "                            conv_name = \"res\" + str(block + 2) + \"a\"\n",
    "                        else:\n",
    "                            conv_name = \"res\" + str(block + 2) + \"b\" + str(i)\n",
    "                    else:\n",
    "                        conv_name = \"res\" + str(block + 2) + chr(97 + i)\n",
    "                    bottleneck_block = self.add_sublayer(\n",
    "                        'bb_%d_%d' %\n",
    "                        (block, i),  #same with PaddleClas, for loading pretrain\n",
    "                        BottleneckBlock(\n",
    "                            in_channels=in_channels\n",
    "                            if i == 0 else out_channels[block] * 4,\n",
    "                            out_channels=out_channels[block],\n",
    "                            stride=2 if i == 0 and block != 0 else 1,\n",
    "                            num_seg=self.num_seg,\n",
    "                            shortcut=shortcut,\n",
    "                            if_first=block == i == 0,\n",
    "                            name=conv_name))\n",
    "                    in_channels = out_channels[block] * 4\n",
    "                    self.block_list.append(bottleneck_block)\n",
    "                    shortcut = True\n",
    "        else:\n",
    "            for block in range(len(depth)):\n",
    "                shortcut = False\n",
    "                for i in range(depth[block]):\n",
    "                    conv_name = \"res\" + str(block + 2) + chr(97 + i)\n",
    "                    basic_block = self.add_sublayer(\n",
    "                        conv_name,\n",
    "                        BasicBlock(in_channels=in_channels[block]\n",
    "                                   if i == 0 else out_channels[block],\n",
    "                                   out_channels=out_channels[block],\n",
    "                                   stride=2 if i == 0 and block != 0 else 1,\n",
    "                                   shortcut=shortcut,\n",
    "                                   name=conv_name))\n",
    "                    self.block_list.append(basic_block)\n",
    "                    shortcut = True\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initiate the parameters.\n",
    "        Note:\n",
    "            1. when indicate pretrained loading path, will load it to initiate backbone.\n",
    "            2. when not indicating pretrained loading path, will follow specific initialization initiate backbone. Always, Conv2D layer will be initiated by KaimingNormal function, and BatchNorm2d will be initiated by Constant function.\n",
    "            Please refer to https://www.paddlepaddle.org.cn/documentation/docs/en/develop/api/paddle/nn/initializer/kaiming/KaimingNormal_en.html\n",
    "        \"\"\"\n",
    "        #XXX: check bias!!! check pretrained!!!\n",
    "\n",
    "        if isinstance(self.pretrained, str) and self.pretrained.strip() != \"\":\n",
    "            load_ckpt(self, self.pretrained)\n",
    "        elif self.pretrained is None or self.pretrained.strip() == \"\":\n",
    "            for layer in self.sublayers():\n",
    "                if isinstance(layer, nn.Conv2D):\n",
    "                    #XXX: no bias\n",
    "                    weight_init_(layer, 'KaimingNormal')\n",
    "                elif isinstance(layer, nn.BatchNorm2D):\n",
    "                    weight_init_(layer, 'Constant', value=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Define how the backbone is going to run.\n",
    "        \"\"\"\n",
    "        #NOTE: Already merge axis 0(batches) and axis 1(channels) before extracting feature phase,\n",
    "        # please refer to paddlevideo/modeling/framework/recognizers/recognizer2d.py#L27\n",
    "        #y = paddle.reshape(\n",
    "        #    inputs, [-1, inputs.shape[2], inputs.shape[3], inputs.shape[4]])\n",
    "\n",
    "        ####ResNet-C: use three 3x3 conv, replace, one 7x7 conv\n",
    "        y = self.conv1_1(inputs)\n",
    "        y = self.conv1_2(y)\n",
    "        y = self.conv1_3(y)\n",
    "\n",
    "        y = self.pool2D_max(y)\n",
    "        for block in self.block_list:\n",
    "            y = block(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Layer):\n",
    "    \"\"\"Cross Entropy Loss.\"\"\"\n",
    "    def __init__(self, loss_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.loss_weight = loss_weight\n",
    "\n",
    "    def _forward(self, score, labels, **kwargs):\n",
    "        \"\"\"Forward function.\n",
    "        Args:\n",
    "            score (paddle.Tensor): The class score.\n",
    "            labels (paddle.Tensor): The ground truth labels.\n",
    "            kwargs: Any keyword argument to be used to calculate\n",
    "                CrossEntropy loss.\n",
    "        Returns:\n",
    "            loss (paddle.Tensor): The returned CrossEntropy loss.\n",
    "        \"\"\"\n",
    "        loss = F.cross_entropy(score, labels, **kwargs)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Defines the computation performed at every call.\n",
    "        Args:\n",
    "            *args: The positional arguments for the corresponding\n",
    "                loss.\n",
    "            **kwargs: The keyword arguments for the corresponding\n",
    "                loss.\n",
    "        Returns:\n",
    "            paddle.Tensor: The calculated loss.\n",
    "        \"\"\"\n",
    "        return self._forward(*args, **kwargs) * self.loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ppTSMHead(nn.Layer):\n",
    "    \"\"\" ppTSM Head\n",
    "    Args:\n",
    "        num_classes (int): The number of classes to be classified.\n",
    "        in_channels (int): The number of channles in input feature.\n",
    "        loss_cfg (dict): Config for building config. Default: dict(name='CrossEntropyLoss').\n",
    "        drop_ratio(float): drop ratio. Default: 0.8.\n",
    "        std(float): Std(Scale) value in normal initilizar. Default: 0.001.\n",
    "        kwargs (dict, optional): Any keyword argument to initialize.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 in_channels,\n",
    "                 drop_ratio=0.8,\n",
    "                 std=0.01,\n",
    "                 data_format=\"NCHW\",\n",
    "                 ls_eps=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.ls_eps = ls_eps\n",
    "\n",
    "        self.fc = Linear(self.in_channels,\n",
    "                         self.num_classes,\n",
    "                         weight_attr=ParamAttr(learning_rate=5.0,\n",
    "                                               regularizer=L2Decay(1e-4)),\n",
    "                         bias_attr=ParamAttr(learning_rate=10.0,\n",
    "                                             regularizer=L2Decay(0.0)))\n",
    "        self.stdv = std\n",
    "        self.loss_func = CrossEntropyLoss()\n",
    "        self.avgpool2d = AdaptiveAvgPool2D((1, 1), data_format=data_format)\n",
    "        self.dropout = Dropout(p=self.drop_ratio)\n",
    "        \n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initiate the FC layer parameters\"\"\"\n",
    "        weight_init_(self.fc, 'Normal', 'fc_0.w_0', 'fc_0.b_0', std=self.stdv)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, seg_num):\n",
    "        \"\"\"Define how the head is going to run.\n",
    "        Args:\n",
    "            x (paddle.Tensor): The input data.\n",
    "            num_segs (int): Number of segments.\n",
    "        Returns:\n",
    "            score: (paddle.Tensor) The classification scores for input samples.\n",
    "        \"\"\"\n",
    "\n",
    "        #XXX: check dropout location!\n",
    "        # [N * num_segs, in_channels, 7, 7]\n",
    "        x = self.avgpool2d(x)\n",
    "        # [N * num_segs, in_channels, 1, 1]\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "            # [N * seg_num, in_channels, 1, 1]\n",
    "        x = paddle.reshape(x, [-1, seg_num, x.shape[1]])\n",
    "        # [N, seg_num, in_channels]\n",
    "        x = paddle.mean(x, axis=1)\n",
    "        # [N, in_channels]\n",
    "        x = paddle.reshape(x, shape=[-1, self.in_channels])\n",
    "        # [N, in_channels]\n",
    "        score = self.fc(x)\n",
    "        # [N, num_class]\n",
    "        return score\n",
    "\n",
    "    def loss(self, scores, labels, valid_mode=False, **kwargs):\n",
    "        \"\"\"Calculate the loss accroding to the model output ```scores```,\n",
    "           and the target ```labels```.\n",
    "        Args:\n",
    "            scores (paddle.Tensor): The output of the model.\n",
    "            labels (paddle.Tensor): The target output of the model.\n",
    "        Returns:\n",
    "            losses (dict): A dict containing field 'loss'(mandatory) and 'top1_acc', 'top5_acc'(optional).\n",
    "        \"\"\"\n",
    "        if len(labels) == 1:  #commonly case\n",
    "            labels = labels[0]\n",
    "            losses = dict()\n",
    "            if self.ls_eps != 0. and not valid_mode:  # label_smooth\n",
    "                loss = self.label_smooth_loss(scores, labels, **kwargs)\n",
    "            else:\n",
    "                loss = self.loss_func(scores, labels, **kwargs)\n",
    "\n",
    "            top1, top5 = self.get_acc(scores, labels, valid_mode)\n",
    "            losses['top1'] = top1\n",
    "            losses['top5'] = top5\n",
    "            losses['loss'] = loss\n",
    "            return losses\n",
    "        elif len(labels) == 3:  # mix_up\n",
    "            labels_a, labels_b, lam = labels\n",
    "            lam = lam[0]  # get lam value\n",
    "            losses = dict()\n",
    "\n",
    "            if self.ls_eps != 0:\n",
    "                loss_a = self.label_smooth_loss(scores, labels_a, **kwargs)\n",
    "                loss_b = self.label_smooth_loss(scores, labels_b, **kwargs)\n",
    "            else:\n",
    "                loss_a = self.loss_func(scores, labels_a, **kwargs)\n",
    "                loss_b = self.loss_func(scores, labels_b, **kwargs)\n",
    "            loss = lam * loss_a + (1 - lam) * loss_b\n",
    "            top1a, top5a = self.get_acc(scores, labels_a, valid_mode)\n",
    "            top1b, top5b = self.get_acc(scores, labels_b, valid_mode)\n",
    "            top1 = lam * top1a + (1 - lam) * top1b\n",
    "            top5 = lam * top5a + (1 - lam) * top5b\n",
    "            losses['top1'] = top1\n",
    "            losses['top5'] = top5\n",
    "            losses['loss'] = loss\n",
    "            return losses\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "    def label_smooth_loss(self, scores, labels, **kwargs):\n",
    "        labels = F.one_hot(labels, self.num_classes)\n",
    "        labels = F.label_smooth(labels, epsilon=self.ls_eps)\n",
    "        labels = paddle.squeeze(labels, axis=1)\n",
    "        loss = self.loss_func(scores, labels, soft_label=True, **kwargs)\n",
    "        return loss\n",
    "\n",
    "    def get_acc(self, scores, labels, valid_mode):\n",
    "        top1 = paddle.metric.accuracy(input=scores, label=labels, k=1)\n",
    "        top5 = paddle.metric.accuracy(input=scores, label=labels, k=5)\n",
    "        return top1, top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Recognizer2D**\n",
    "\n",
    "将主干网络和头部分封装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Recognizer2D(nn.Layer):\n",
    "    \"\"\"2D recognizer model framework.\"\"\"\n",
    "    def __init__(self, backbone=None, head=None):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.backbone.init_weights()\n",
    "        self.head = head\n",
    "        self.head.init_weights()\n",
    "\n",
    "    def forward_net(self, imgs):\n",
    "        # NOTE: As the num_segs is an attribute of dataset phase, and didn't pass to build_head phase, should obtain it from imgs(paddle.Tensor) now, then call self.head method.\n",
    "        num_segs = imgs.shape[1]  # imgs.shape=[N,T,C,H,W], for most commonly case\n",
    "        imgs = paddle.reshape_(imgs, [-1] + list(imgs.shape[2:]))\n",
    "        feature = self.backbone(imgs)\n",
    "        cls_score = self.head(feature, num_segs)\n",
    "        return cls_score\n",
    "\n",
    "    def train_step(self, data_batch):\n",
    "        \"\"\"Define how the model is going to train, from input to output.\n",
    "        \"\"\"\n",
    "        imgs = data_batch[0]\n",
    "        labels = data_batch[1:]\n",
    "        cls_score = self.forward_net(imgs)\n",
    "        loss_metrics = self.head.loss(cls_score, labels)\n",
    "        return loss_metrics\n",
    "\n",
    "    def val_step(self, data_batch):\n",
    "        imgs = data_batch[0]\n",
    "        labels = data_batch[1:]\n",
    "        cls_score = self.forward_net(imgs)\n",
    "        loss_metrics = self.head.loss(cls_score, labels, valid_mode=True)\n",
    "        return loss_metrics\n",
    "\n",
    "    def test_step(self, data_batch):\n",
    "        \"\"\"Define how the model is going to test, from input to output.\"\"\"\n",
    "        # NOTE: (shipping) when testing, the net won't call head.loss, we deal with the test processing in /paddlevideo/metrics\n",
    "        imgs = data_batch[0]\n",
    "        cls_score = self.forward_net(imgs)\n",
    "        return cls_score\n",
    "\n",
    "    def infer_step(self, data_batch):\n",
    "        \"\"\"Define how the model is going to test, from input to output.\"\"\"\n",
    "        imgs = data_batch[0]\n",
    "        cls_score = self.forward_net(imgs)\n",
    "        return cls_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_ckpt(model, weight_path):\n",
    "    \"\"\"\n",
    "    1. Load pre-trained model parameters\n",
    "    2. Extract and convert from the pre-trained model to the parameters \n",
    "    required by the existing model\n",
    "    3. Load the converted parameters of the existing model\n",
    "    \"\"\"\n",
    "    #model.set_state_dict(state_dict)\n",
    "\n",
    "    if not osp.isfile(weight_path):\n",
    "        raise IOError(f'{weight_path} is not a checkpoint file')\n",
    "    #state_dicts = load(weight_path)\n",
    "\n",
    "    state_dicts = paddle.load(weight_path)\n",
    "    tmp = {}\n",
    "    total_len = len(model.state_dict())\n",
    "    with tqdm(total=total_len, position=1, bar_format='{desc}', desc=\"Loading weights\") as desc:\n",
    "        for item in tqdm(model.state_dict(), total=total_len, position=0):\n",
    "            name = item\n",
    "            desc.set_description('Loading %s' % name)\n",
    "            if name not in state_dicts: # Convert from non-parallel model\n",
    "                if str('backbone.' + name) in state_dicts:\n",
    "                    tmp[name] = state_dicts['backbone.' + name]\n",
    "            else:  # Convert from parallel model\n",
    "                tmp[name] = state_dicts[name]\n",
    "            time.sleep(0.01)\n",
    "    ret_str = \"loading {:<20d} weights completed.\".format(len(model.state_dict()))\n",
    "    desc.set_description(ret_str)\n",
    "    model.set_state_dict(tmp)\n",
    "\n",
    "def weight_init_(layer,\n",
    "                 func,\n",
    "                 weight_name=None,\n",
    "                 bias_name=None,\n",
    "                 bias_value=0.0,\n",
    "                 **kwargs):\n",
    "    \"\"\"\n",
    "    In-place params init function.\n",
    "    Usage:\n",
    "    .. code-block:: python\n",
    "        import paddle\n",
    "        import numpy as np\n",
    "        data = np.ones([3, 4], dtype='float32')\n",
    "        linear = paddle.nn.Linear(4, 4)\n",
    "        input = paddle.to_tensor(data)\n",
    "        print(linear.weight)\n",
    "        linear(input)\n",
    "        weight_init_(linear, 'Normal', 'fc_w0', 'fc_b0', std=0.01, mean=0.1)\n",
    "        print(linear.weight)\n",
    "    \"\"\"\n",
    "\n",
    "    if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "        getattr(init, func)(**kwargs)(layer.weight)\n",
    "        if weight_name is not None:\n",
    "            # override weight name\n",
    "            layer.weight.name = weight_name\n",
    "\n",
    "    if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "        init.Constant(bias_value)(layer.bias)\n",
    "        if bias_name is not None:\n",
    "            # override bias name\n",
    "            layer.bias.name = bias_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3 训练配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用pptsm在Kinetics-400数据集上训练的模型作为预训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-04 19:47:07--  https://videotag.bj.bcebos.com/PaddleVideo-release2.1/PPTSM/ppTSM_k400_uniform_distill.pdparams\n",
      "Resolving videotag.bj.bcebos.com (videotag.bj.bcebos.com)... 124.237.208.76, 2409:8c04:1001:1002:0:ff:b001:368a\n",
      "Connecting to videotag.bj.bcebos.com (videotag.bj.bcebos.com)|124.237.208.76|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "Length: 147975812 (141M) [application/octet-stream]\n",
      "Saving to: ‘ppTSM_k400_uniform_distill.pdparams’\n",
      "\n",
      "ppTSM_k400_uniform_ 100%[===================>] 141.12M  45.3MB/s    in 13s     \n",
      "\n",
      "2023-08-04 19:47:21 (10.7 MB/s) - ‘ppTSM_k400_uniform_distill.pdparams’ saved [147975812/147975812]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 仅在第一次运行代码时使用\n",
    "!wget https://videotag.bj.bcebos.com/PaddleVideo-release2.1/PPTSM/ppTSM_k400_uniform_distill.pdparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "framework = 'Recognizer2D'\n",
    "\n",
    "# backbone \n",
    "# name = 'ResNetTweaksTSM'\n",
    "#pretrained = 'ResNet50_vd_ssld_v2_pretrained.pdparams'\n",
    "pretrained = 'ppTSM_k400_uniform_distill.pdparams'\n",
    "depth = 50\n",
    "\n",
    "# head\n",
    "num_classes = 101\n",
    "in_channels = 2048\n",
    "drop_ratio = 0.5\n",
    "std = 0.01\n",
    "ls_eps = 0.1\n",
    "\n",
    "# DATASET\n",
    "# batch_size = 16\n",
    "batch_size = 8\n",
    "num_workers = 1\n",
    "test_batch_size = 1\n",
    "# train_file_path = 'ucf101_train_videos.txt'\n",
    "# valid_file_path = 'ucf101_val_videos.txt'\n",
    "train_file_path = '/workspaces/data/UCF101/ucf101_train_split_1_videos.txt'\n",
    "valid_file_path = '/workspaces/data/UCF101/ucf101_val_split_1_videos.txt'\n",
    "suffix = '.avi'\n",
    "train_shuffle = True\n",
    "valid_shuffle = False\n",
    "\n",
    "# mixup\n",
    "mix_collate_fn = Mixup(alpha=0.2)\n",
    "\n",
    "# OPTIMIZER\n",
    "momentum = 0.9\n",
    "\n",
    "# lr\n",
    "max_epoch = 30 \n",
    "warmup_epochs = 5\n",
    "warmup_start_lr = 0.0005\n",
    "cosine_base_lr = 0.001\n",
    "iter_step = True\n",
    "\n",
    "\n",
    "# PRECISEBN\n",
    "preciseBN = True\n",
    "preciseBN_interval = 5\n",
    "num_iters_preciseBN = 200\n",
    "\n",
    "model_name = 'ppTSM'\n",
    "log_interval = 10\n",
    "save_interval = 3\n",
    "val_interval = 1\n",
    "epochs = 30 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Color = {\n",
    "    'RED': '\\033[31m',\n",
    "    'HEADER': '\\033[35m',  # deep purple\n",
    "    'PURPLE': '\\033[95m',  # purple\n",
    "    'OKBLUE': '\\033[94m',\n",
    "    'OKGREEN': '\\033[92m',\n",
    "    'WARNING': '\\033[93m',\n",
    "    'FAIL': '\\033[91m',\n",
    "    'ENDC': '\\033[0m'\n",
    "}\n",
    "\n",
    "\n",
    "def coloring(message, color=\"OKGREEN\"):\n",
    "    assert color in Color.keys()\n",
    "    if os.environ.get('COLORING', True):\n",
    "        return Color[color] + str(message) + Color[\"ENDC\"]\n",
    "    else:\n",
    "        return message\n",
    "\n",
    "def build_record(framework_type):\n",
    "    record_list = [\n",
    "        (\"loss\", AverageMeter('loss', '7.5f')),\n",
    "        (\"lr\", AverageMeter('lr', 'f', need_avg=False)),\n",
    "        (\"batch_time\", AverageMeter('elapse', '.3f')),\n",
    "        (\"reader_time\", AverageMeter('reader', '.3f')),\n",
    "    ]\n",
    "\n",
    "    if 'Recognizer' in framework_type:\n",
    "        record_list.append((\"top1\", AverageMeter(\"top1\", '.5f')))\n",
    "        record_list.append((\"top5\", AverageMeter(\"top5\", '.5f')))\n",
    "\n",
    "    record_list = OrderedDict(record_list)\n",
    "    return record_list\n",
    "\n",
    "def log_batch(metric_list, batch_id, epoch_id, total_epoch, mode, ips):\n",
    "    metric_str = ' '.join([str(m.value) for m in metric_list.values()])\n",
    "    epoch_str = \"epoch:[{:>3d}/{:<3d}]\".format(epoch_id, total_epoch)\n",
    "    step_str = \"{:s} step:{:<4d}\".format(mode, batch_id)\n",
    "    print(\"{:s} {:s} {:s}s {}\".format(\n",
    "        coloring(epoch_str, \"HEADER\") if batch_id == 0 else epoch_str,\n",
    "        coloring(step_str, \"PURPLE\"), coloring(metric_str, 'OKGREEN'), ips))\n",
    "\n",
    "\n",
    "def log_epoch(metric_list, epoch, mode, ips):\n",
    "    metric_avg = ' '.join([str(m.mean) for m in metric_list.values()] +\n",
    "                          [metric_list['batch_time'].total])\n",
    "\n",
    "    end_epoch_str = \"END epoch:{:<3d}\".format(epoch)\n",
    "\n",
    "    print(\"{:s} {:s} {:s}s {}\".format(coloring(end_epoch_str, \"RED\"),\n",
    "                                            coloring(mode, \"PURPLE\"),\n",
    "                                            coloring(metric_avg, \"OKGREEN\"),\n",
    "                                            ips))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self, name='', fmt='f', need_avg=True):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.need_avg = need_avg\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset \"\"\"\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        \"\"\" update \"\"\"\n",
    "        if isinstance(val, paddle.Tensor):\n",
    "            val = val.numpy()[0]\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    @property\n",
    "    def total(self):\n",
    "        return '{self.name}_sum: {self.sum:{self.fmt}}'.format(self=self)\n",
    "\n",
    "    @property\n",
    "    def total_minute(self):\n",
    "        return '{self.name}_sum: {s:{self.fmt}} min'.format(s=self.sum / 60,\n",
    "                                                            self=self)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return '{self.name}_avg: {self.avg:{self.fmt}}'.format(\n",
    "            self=self) if self.need_avg else ''\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return '{self.name}: {self.val:{self.fmt}}'.format(self=self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomWarmupCosineDecay(LRScheduler):\n",
    "    \"\"\"\n",
    "    We combine warmup and stepwise-cosine which is used in slowfast model.\n",
    "    Args:\n",
    "        warmup_start_lr (float): start learning rate used in warmup stage.\n",
    "        warmup_epochs (int): the number epochs of warmup.\n",
    "        cosine_base_lr (float|int, optional): base learning rate in cosine schedule.\n",
    "        max_epoch (int): total training epochs.\n",
    "        num_iters(int): number iterations of each epoch.\n",
    "        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n",
    "        verbose (bool, optional): If ``True``, prints a message to stdout for each update. Default: ``False`` .\n",
    "    Returns:\n",
    "        ``CosineAnnealingDecay`` instance to schedule learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 warmup_start_lr,\n",
    "                 warmup_epochs,\n",
    "                 cosine_base_lr,\n",
    "                 max_epoch,\n",
    "                 num_iters,\n",
    "                 last_epoch=-1,\n",
    "                 verbose=False):\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.cosine_base_lr = cosine_base_lr\n",
    "        self.max_epoch = max_epoch\n",
    "        self.num_iters = num_iters\n",
    "        #call step() in base class, last_lr/last_epoch/base_lr will be update\n",
    "        super(CustomWarmupCosineDecay, self).__init__(last_epoch=last_epoch,\n",
    "                                                      verbose=verbose)\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        \"\"\"\n",
    "        ``step`` should be called after ``optimizer.step`` . It will update the learning rate in optimizer according to current ``epoch`` .\n",
    "        The new learning rate will take effect on next ``optimizer.step`` .\n",
    "        Args:\n",
    "            epoch (int, None): specify current epoch. Default: None. Auto-increment from last_epoch=-1.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if epoch is None:\n",
    "            if self.last_epoch == -1:\n",
    "                self.last_epoch += 1\n",
    "            else:\n",
    "                self.last_epoch += 1 / self.num_iters  # update step with iters\n",
    "        else:\n",
    "            self.last_epoch = epoch\n",
    "        self.last_lr = self.get_lr()\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Epoch {}: {} set learning rate to {}.'.format(\n",
    "                self.last_epoch, self.__class__.__name__, self.last_lr))\n",
    "\n",
    "    def _lr_func_cosine(self, cur_epoch, cosine_base_lr, max_epoch):\n",
    "        return cosine_base_lr * (math.cos(math.pi * cur_epoch / max_epoch) +\n",
    "                                 1.0) * 0.5\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Define lr policy\"\"\"\n",
    "        lr = self._lr_func_cosine(self.last_epoch, self.cosine_base_lr,\n",
    "                                  self.max_epoch)\n",
    "        lr_end = self._lr_func_cosine(self.warmup_epochs, self.cosine_base_lr,\n",
    "                                      self.max_epoch)\n",
    "\n",
    "        # Perform warm up.\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            lr_start = self.warmup_start_lr\n",
    "            alpha = (lr_end - lr_start) / self.warmup_epochs\n",
    "            lr = self.last_epoch * alpha + lr_start\n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def do_preciseBN(model, data_loader, parallel, num_iters=200):\n",
    "    \"\"\"\n",
    "    Recompute and update the batch norm stats to make them more precise. During\n",
    "    training both BN stats and the weight are changing after every iteration, so\n",
    "    the running average can not precisely reflect the actual stats of the\n",
    "    current model.\n",
    "    In this function, the BN stats are recomputed with fixed weights, to make\n",
    "    the running average more precise. Specifically, it computes the true average\n",
    "    of per-batch mean/variance instead of the running average.\n",
    "    This is useful to improve validation accuracy.\n",
    "    Args:\n",
    "        model: the model whose bn stats will be recomputed\n",
    "        data_loader: an iterator. Produce data as input to the model\n",
    "        num_iters: number of iterations to compute the stats.\n",
    "    Return:\n",
    "        the model with precise mean and variance in bn layers.\n",
    "    \"\"\"\n",
    "    bn_layers_list = [\n",
    "        m for m in model.sublayers()\n",
    "        if any((isinstance(m, bn_type)\n",
    "                for bn_type in (paddle.nn.BatchNorm1D, paddle.nn.BatchNorm2D,\n",
    "                                paddle.nn.BatchNorm3D))) and m.training\n",
    "    ]\n",
    "    if len(bn_layers_list) == 0:\n",
    "        return\n",
    "\n",
    "    # moving_mean=moving_mean*momentum+batch_mean*(1.−momentum)\n",
    "    # we set momentum=0. to get the true mean and variance during forward\n",
    "    momentum_actual = [bn._momentum for bn in bn_layers_list]\n",
    "    for bn in bn_layers_list:\n",
    "        bn._momentum = 0.\n",
    "\n",
    "    running_mean = [paddle.zeros_like(bn._mean)\n",
    "                    for bn in bn_layers_list]  #pre-ignore\n",
    "    running_var = [paddle.zeros_like(bn._variance) for bn in bn_layers_list]\n",
    "\n",
    "    ind = -1\n",
    "    for ind, data in enumerate(itertools.islice(data_loader, num_iters)):\n",
    "        print(\"doing precise BN {} / {}...\".format(ind + 1, num_iters))\n",
    "        if parallel:\n",
    "            model._layers.train_step(data)\n",
    "        else:\n",
    "            model.train_step(data)\n",
    "\n",
    "        for i, bn in enumerate(bn_layers_list):\n",
    "            # Accumulates the bn stats.\n",
    "            running_mean[i] += (bn._mean - running_mean[i]) / (ind + 1)\n",
    "            running_var[i] += (bn._variance - running_var[i]) / (ind + 1)\n",
    "\n",
    "    assert ind == num_iters - 1, (\n",
    "        \"update_bn_stats is meant to run for {} iterations, but the dataloader stops at {} iterations.\"\n",
    "        .format(num_iters, ind))\n",
    "\n",
    "    # Sets the precise bn stats.\n",
    "    for i, bn in enumerate(bn_layers_list):\n",
    "        bn._mean.set_value(running_mean[i])\n",
    "        bn._variance.set_value(running_var[i])\n",
    "        bn._momentum = momentum_actual[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(validate=True):\n",
    "    \"\"\"Train model entry\n",
    "    Args:\n",
    "        weights (str): weights path for finetuning.\n",
    "        validate (bool): Whether to do evaluation. Default: False.\n",
    "    \"\"\"\n",
    "    output_dir = f\"./output/{model_name}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        try:\n",
    "            os.makedirs(dir)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    places = paddle.set_device('gpu')\n",
    "    \n",
    "    # 1. Construct model\n",
    "    pptsm = ResNetTweaksTSM(pretrained=pretrained, depth=depth)\n",
    "    head = ppTSMHead(num_classes=num_classes,\n",
    "                     in_channels=in_channels,\n",
    "                     drop_ratio=drop_ratio,\n",
    "                     std=std,\n",
    "                     ls_eps=ls_eps)\n",
    "\n",
    "    model = Recognizer2D(backbone=pptsm, head=head)\n",
    "\n",
    "    # 2. Construct dataset and dataloader\n",
    "    train_pipeline = Compose(train_mode=True)\n",
    "    train_dataset = VideoDataset(file_path=train_file_path,\n",
    "                                 pipeline=train_pipeline,\n",
    "                                 suffix=suffix)\n",
    "    train_loader = build_dataloader(dataset=train_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    num_workers=num_workers,\n",
    "                                    train_mode=True,\n",
    "                                    places=places,\n",
    "                                    shuffle=train_shuffle,\n",
    "                                    collate_fn_cfg=mix_collate_fn)                             \n",
    "\n",
    "    if validate:\n",
    "        valid_pipeline = Compose(train_mode=False)\n",
    "        valid_dataset = VideoDataset(file_path=valid_file_path,\n",
    "                                     pipeline=valid_pipeline,\n",
    "                                     suffix=suffix)\n",
    "        valid_loader = build_dataloader(dataset=valid_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        num_workers=num_workers,\n",
    "                                        train_mode=False,\n",
    "                                        places=places,\n",
    "                                        shuffle=valid_shuffle,\n",
    "                                        collate_fn_cfg=None)\n",
    "\n",
    "    # 3. Construct solver.\n",
    "    lr = CustomWarmupCosineDecay(warmup_start_lr=warmup_start_lr,\n",
    "                                 warmup_epochs=warmup_epochs,\n",
    "                                 cosine_base_lr=cosine_base_lr,\n",
    "                                 max_epoch=max_epoch,\n",
    "                                 num_iters=1)\n",
    "    optimizer = paddle.optimizer.Momentum(\n",
    "        learning_rate=lr,\n",
    "        momentum=momentum,\n",
    "        parameters=model.parameters(),\n",
    "        use_nesterov=True,\n",
    "        weight_decay=paddle.regularizer.L2Decay(coeff=1e-4))\n",
    "\n",
    "    # 4. Train Model\n",
    "    best = 0.\n",
    "    for epoch in range(0, epochs):\n",
    "        model.train()\n",
    "        record_list = build_record(framework)\n",
    "        tic = time.time()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            record_list['reader_time'].update(time.time() - tic)\n",
    "\n",
    "            # 4.1 forward\n",
    "            outputs = model.train_step(data)\n",
    "\n",
    "            # 4.2 backward\n",
    "            avg_loss = outputs['loss']\n",
    "            avg_loss.backward()\n",
    "\n",
    "            # 4.3 minimize\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            # log record\n",
    "            record_list['lr'].update(optimizer.get_lr(), batch_size)\n",
    "            for name, value in outputs.items():\n",
    "                record_list[name].update(value, batch_size)\n",
    "\n",
    "            record_list['batch_time'].update(time.time() - tic)\n",
    "            tic = time.time()\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                ips = \"ips: {:.5f} instance/sec.\".format(\n",
    "                    batch_size / record_list[\"batch_time\"].val)\n",
    "                log_batch(record_list, i, epoch + 1, epochs, \"train\", ips)\n",
    "\n",
    "            # learning rate iter step\n",
    "            if iter_step:\n",
    "                lr.step()\n",
    "\n",
    "        ips = \"avg_ips: {:.5f} instance/sec.\".format(\n",
    "            batch_size * record_list[\"batch_time\"].count /\n",
    "            record_list[\"batch_time\"].sum)\n",
    "        log_epoch(record_list, epoch + 1, \"train\", ips)\n",
    "    \n",
    "        def evaluate(best):\n",
    "            model.eval()\n",
    "            record_list = build_record(framework)\n",
    "            record_list.pop('lr')\n",
    "            tic = time.time()\n",
    "            for i, data in enumerate(valid_loader):\n",
    "                outputs = model.val_step(data)\n",
    "                \n",
    "                # log_record\n",
    "                for name, value in outputs.items():\n",
    "                    record_list[name].update(value, batch_size)\n",
    "\n",
    "                record_list['batch_time'].update(time.time() - tic)\n",
    "                tic = time.time()\n",
    "\n",
    "                if i % log_interval == 0:\n",
    "                    ips = \"ips: {:.5f} instance/sec.\".format(\n",
    "                        batch_size / record_list[\"batch_time\"].val)\n",
    "                    log_batch(record_list, i, epoch + 1, epochs, \"val\", ips)\n",
    "\n",
    "            ips = \"avg_ips: {:.5f} instance/sec.\".format(\n",
    "                batch_size * record_list[\"batch_time\"].count /\n",
    "                record_list[\"batch_time\"].sum)\n",
    "            log_epoch(record_list, epoch + 1, \"val\", ips)\n",
    "\n",
    "            best_flag = False\n",
    "            for top_flag in ['hit_at_one', 'top1']:\n",
    "                if record_list.get(\n",
    "                        top_flag) and record_list[top_flag].avg > best:\n",
    "                    best = record_list[top_flag].avg\n",
    "                    best_flag = True\n",
    "\n",
    "            return best, best_flag\n",
    "        \n",
    "        # use precise bn to improve acc\n",
    "        if preciseBN and (epoch % preciseBN_interval == 0 or epoch == epochs - 1):\n",
    "            do_preciseBN(\n",
    "                model, train_loader, False,\n",
    "                min(num_iters_preciseBN, len(train_loader)))\n",
    "\n",
    "        # 5. Validation\n",
    "        if validate and (epoch % val_interval == 0\n",
    "                         or epoch == epochs - 1):\n",
    "            with paddle.no_grad():\n",
    "                best, save_best_flag = evaluate(best)\n",
    "            # save best\n",
    "            if save_best_flag:\n",
    "                paddle.save(optimizer.state_dict(),\n",
    "                     osp.join(output_dir, model_name + \"_best.pdopt\"))\n",
    "                paddle.save(model.state_dict(),\n",
    "                     osp.join(output_dir, model_name + \"_best.pdparams\"))\n",
    "                print(\n",
    "                    f\"Already save the best model (top1 acc){int(best *10000)/10000}\"\n",
    "                )\n",
    "\n",
    "        # 6. Save model and optimizer\n",
    "        if epoch % save_interval == 0 or epoch == epochs - 1:\n",
    "            paddle.save(\n",
    "                optimizer.state_dict(),\n",
    "                osp.join(output_dir,\n",
    "                         model_name + f\"_epoch_{epoch+1:05d}.pdopt\"))\n",
    "            paddle.save(\n",
    "                model.state_dict(),\n",
    "                osp.join(output_dir,\n",
    "                         model_name + f\"_epoch_{epoch+1:05d}.pdparams\"))\n",
    "    print(f'training {model_name} finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0804 19:47:21.706238  1840 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 12.2, Runtime API Version: 11.8\n",
      "W0804 19:47:21.707000  1840 gpu_resources.cc:149] device: 0, cuDNN Version: 8.9.\n",
      "100%|██████████| 275/275 [00:03<00:00, 90.44it/s]\n",
      "Loading bb_3_2.conv2._batch_norm._variance: \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ucf101_train_videos.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 在执行代码过程中，如果出现 ‘ValueError: parameter name [conv1_weights] have be been used’ 问题，\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# 可以点击上方的第三个按钮 ‘重启并运行全部’ 来解决\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_model(\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[28], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(validate)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# 2. Construct dataset and dataloader\u001b[39;00m\n\u001b[1;32m     27\u001b[0m train_pipeline \u001b[39m=\u001b[39m Compose(train_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m train_dataset \u001b[39m=\u001b[39m VideoDataset(file_path\u001b[39m=\u001b[39;49mtrain_file_path,\n\u001b[1;32m     29\u001b[0m                              pipeline\u001b[39m=\u001b[39;49mtrain_pipeline,\n\u001b[1;32m     30\u001b[0m                              suffix\u001b[39m=\u001b[39;49msuffix)\n\u001b[1;32m     31\u001b[0m train_loader \u001b[39m=\u001b[39m build_dataloader(dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[1;32m     32\u001b[0m                                 batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m     33\u001b[0m                                 num_workers\u001b[39m=\u001b[39mnum_workers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                 shuffle\u001b[39m=\u001b[39mtrain_shuffle,\n\u001b[1;32m     37\u001b[0m                                 collate_fn_cfg\u001b[39m=\u001b[39mmix_collate_fn)                             \n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m validate:\n",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36mVideoDataset.__init__\u001b[0;34m(self, file_path, pipeline, num_retries, suffix, test_mode)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_retries \u001b[39m=\u001b[39m num_retries\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuffix \u001b[39m=\u001b[39m suffix\n\u001b[0;32m---> 23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_file()\n\u001b[1;32m     24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_mode \u001b[39m=\u001b[39m test_mode\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mVideoDataset.load_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load index file to get video information.\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m info \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 29\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m     30\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m fin:\n\u001b[1;32m     31\u001b[0m         line_split \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ucf101_train_videos.txt'"
     ]
    }
   ],
   "source": [
    "# 在执行代码过程中，如果出现 ‘ValueError: parameter name [conv1_weights] have be been used’ 问题，\n",
    "# 可以点击上方的第三个按钮 ‘重启并运行全部’ 来解决\n",
    "\n",
    "train_model(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.5 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CenterCropMetric(object):\n",
    "    def __init__(self, data_size, batch_size, log_interval=20):\n",
    "        \"\"\"prepare for metrics\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_size = data_size\n",
    "        self.batch_size = batch_size\n",
    "        self.log_interval = log_interval\n",
    "        self.top1 = []\n",
    "        self.top5 = []\n",
    "\n",
    "    def update(self, batch_id, data, outputs):\n",
    "        \"\"\"update metrics during each iter\n",
    "        \"\"\"\n",
    "        labels = data[1]\n",
    "\n",
    "        top1 = paddle.metric.accuracy(input=outputs, label=labels, k=1)\n",
    "        top5 = paddle.metric.accuracy(input=outputs, label=labels, k=5)\n",
    "\n",
    "        self.top1.append(top1.numpy())\n",
    "        self.top5.append(top5.numpy())\n",
    "        # preds ensemble\n",
    "        if batch_id % self.log_interval == 0:\n",
    "            print(\"[TEST] Processing batch {}/{} ...\".format(batch_id, self.data_size // self.batch_size))\n",
    "\n",
    "    def accumulate(self):\n",
    "        \"\"\"accumulate metrics when finished all iters.\n",
    "        \"\"\"\n",
    "        print('[TEST] finished, avg_acc1= {}, avg_acc5= {} '.format(\n",
    "            np.mean(np.array(self.top1)), np.mean(np.array(self.top5)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model(weights):\n",
    "    places = paddle.set_device('gpu')\n",
    "    \n",
    "    # 1. Construct model\n",
    "    pptsm = ResNetTweaksTSM(pretrained=None, depth=depth)\n",
    "    head = ppTSMHead(num_classes=num_classes,\n",
    "                     in_channels=in_channels,\n",
    "                     drop_ratio=drop_ratio,\n",
    "                     std=std,\n",
    "                     ls_eps=ls_eps)\n",
    "\n",
    "    model = Recognizer2D(backbone=pptsm, head=head)\n",
    "    \n",
    "    # 2. Construct dataset and dataloader\n",
    "    test_pipeline = Compose(train_mode=False)\n",
    "    test_dataset = VideoDataset(file_path=valid_file_path,\n",
    "                                 pipeline=test_pipeline,\n",
    "                                 suffix=suffix)\n",
    "    test_sampler = paddle.io.DistributedBatchSampler(test_dataset,\n",
    "                                                     batch_size=test_batch_size,\n",
    "                                                     shuffle=valid_shuffle,\n",
    "                                                     drop_last=False)\n",
    "    test_loader = paddle.io.DataLoader(test_dataset,\n",
    "                                       batch_sampler=test_sampler,\n",
    "                                       places=places,\n",
    "                                       return_list=True)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    state_dicts = paddle.load(weights)\n",
    "    model.set_state_dict(state_dicts)\n",
    "\n",
    "    # add params to metrics\n",
    "    data_size = len(test_dataset)\n",
    "    \n",
    "    metric = CenterCropMetric(data_size=data_size, batch_size=test_batch_size)\n",
    "    for batch_id, data in enumerate(test_loader):\n",
    "        outputs = model.test_step(data)\n",
    "        metric.update(batch_id, data, outputs)\n",
    "    metric.accumulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 在执行代码过程中，如果出现 ‘ValueError: parameter name [conv1_weights] have be been used’ 问题，\n",
    "# 可以点击上方的第三个按钮 ‘重启并运行全部’ 来解决\n",
    "model_file = './output/ppTSM/ppTSM_best.pdparams'\n",
    "test_model(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.6 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference():\n",
    "    model_file = './output/ppTSM/ppTSM_best.pdparams'\n",
    "\n",
    "    # 1. Construct model\n",
    "    pptsm = ResNetTweaksTSM(pretrained=None, depth=depth)\n",
    "    head = ppTSMHead(num_classes=num_classes,\n",
    "                     in_channels=in_channels,\n",
    "                     drop_ratio=drop_ratio,\n",
    "                     std=std,\n",
    "                     ls_eps=ls_eps)\n",
    "\n",
    "    model = Recognizer2D(backbone=pptsm, head=head)\n",
    "\n",
    "    # 2. Construct dataset and dataloader\n",
    "    test_pipeline = Compose(train_mode=False)\n",
    "    test_dataset = VideoDataset(file_path=valid_file_path,\n",
    "                                 pipeline=test_pipeline,\n",
    "                                 suffix=suffix)\n",
    "    test_sampler = paddle.io.DistributedBatchSampler(test_dataset,\n",
    "                                                     batch_size=10,\n",
    "                                                     shuffle=True,\n",
    "                                                     drop_last=False)\n",
    "    test_loader = paddle.io.DataLoader(test_dataset,\n",
    "                                       batch_sampler=test_sampler,\n",
    "                                       places=paddle.set_device('gpu'),\n",
    "                                       return_list=True)\n",
    "\n",
    "    model.eval()\n",
    "    state_dicts = paddle.load(model_file)\n",
    "    model.set_state_dict(state_dicts)\n",
    "\n",
    "    for batch_id, data in enumerate(test_loader):\n",
    "        labels = data[1]\n",
    "        outputs = model.test_step(data)\n",
    "        scores = F.softmax(outputs)\n",
    "        class_id = paddle.argmax(scores, axis=-1)\n",
    "        pred = class_id.numpy()[0]\n",
    "        label = labels.numpy()[0][0]\n",
    "        \n",
    "        print('真实类别：{}, 模型预测类别：{}'.format(pred, label))\n",
    "        if batch_id > 5:\n",
    "            break\n",
    "\n",
    "# 启动推理\n",
    "# 在执行代码过程中，如果出现 ‘ValueError: parameter name [conv1_weights] have be been used’ 问题，\n",
    "# 可以点击上方的第三个按钮 ‘重启并运行全部’ 来解决\n",
    "inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. 实验总结\n",
    "本章提供了一个基于百度 AIStudio 平台实现的视频分类实验。本章对案例做了详尽的剖析，阐明了整个实验功能、结构与流程的设计，详细解释了如何处理数据、构建视频分类模型以及训练模型。训练好的模型在多条数据上进行测试，结果表明模型具有较快的推断速度和较好的分类性能。读者可以在该案例的基础上开发更有针对性的应用案例。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
